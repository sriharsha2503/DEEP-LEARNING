{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EEKagzwC2eW",
        "outputId": "e2ec956b-24ba-4004-9d4d-22642052c3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 129MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 37.2MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 33.2MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.30MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0264\n",
            "Epoch 2, Loss: 0.0183\n",
            "Epoch 3, Loss: 0.0141\n",
            "Epoch 4, Loss: 0.0139\n",
            "Epoch 5, Loss: 0.0108\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Autoencoder architecture\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 784),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# MNIST Dataloader\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoEncoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "\n",
        "for epoch in range(5):\n",
        "    for batch in train_loader:\n",
        "        imgs, _ = batch\n",
        "        imgs = imgs.view(imgs.size(0), -1).to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, imgs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Variational Autoencoder Model\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.Encoder = nn.Sequential(\n",
        "            nn.Linear(784, 256, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128, bias=True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.mean = nn.Linear(128, 10, bias=True)\n",
        "        self.std = nn.Linear(128, 10, bias=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.Decoder = nn.Sequential(\n",
        "            nn.Linear(10, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 784, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.Encoder(x)\n",
        "        mean = self.mean(enc)\n",
        "        std = self.std(enc)\n",
        "        z = mean + std * torch.randn_like(std)\n",
        "        y = self.Decoder(z)\n",
        "        return mean, std, y.view(-1, 1, 28, 28)\n",
        "\n",
        "# Loss Function for VAE\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    reproduction_loss = F.binary_cross_entropy(x_hat.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "# Train for one epoch\n",
        "def train_one_epoch(epoch_index):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for i, data in enumerate(train_data_loader):\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        mean, std, outputs = model(inputs.view(-1, 784))\n",
        "        loss = loss_function(inputs, outputs, mean, std)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch_index+1}, Loss: {total_loss / (len(train_data_loader) * batch_size):.4f}\")\n",
        "\n",
        "# Instantiate model, optimizer\n",
        "model = VariationalAutoEncoder().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    train_one_epoch(epoch)\n",
        "\n",
        "# Generate synthetic digits\n",
        "def generate_digit():\n",
        "    model.eval()  # Switch to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        mean = torch.zeros((1, 10)).to(device)\n",
        "        var = torch.ones((1, 10)).to(device)\n",
        "        z_sample = mean + var * torch.randn_like(var)\n",
        "        x_decoded = model.Decoder(z_sample)\n",
        "        digit = x_decoded.detach().cpu().reshape(28, 28)\n",
        "        plt.imshow(digit, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Generate and visualize two digits\n",
        "generate_digit()\n",
        "generate_digit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "cGWlLNiNIaJV",
        "outputId": "5950b053-6922-4d30-9c6a-a71a8ee5372c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 146.0310\n",
            "Epoch 2, Loss: 111.0002\n",
            "Epoch 3, Loss: 104.7004\n",
            "Epoch 4, Loss: 101.3404\n",
            "Epoch 5, Loss: 99.0346\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}