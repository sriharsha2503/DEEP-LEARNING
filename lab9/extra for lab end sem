import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt

# --- Custom Synthetic Dataset ---
class SyntheticTimeSeriesDataset(Dataset):
    def __init__(self, num_samples=100, seq_length=20, num_features=1, random_seed=42):
        self.features, self.targets = self._generate_data(num_samples, seq_length, num_features, random_seed)
        self.features = torch.tensor(self.features, dtype=torch.float32)
        self.targets = torch.tensor(self.targets, dtype=torch.float32)

    def _generate_data(self, num_samples, seq_length, num_features, random_seed):
        np.random.seed(random_seed)
        t = np.linspace(0, 1, seq_length)
        base_signals = []
        for i in range(num_features):
            freq = np.random.uniform(1, 5)
            phase = np.random.uniform(0, 2 * np.pi)
            amplitude = np.random.uniform(0.5, 2)
            signal = amplitude * np.sin(2 * np.pi * freq * t + phase)
            base_signals.append(signal)

        features = np.zeros((num_samples, seq_length, num_features))
        targets = np.zeros((num_samples, 1))
        for i in range(num_samples):
            for j in range(num_features):
                noise = np.random.normal(0, 0.1, seq_length)
                features[i, :, j] = base_signals[j] + noise
            targets[i, 0] = np.mean(features[i, -1, :]) + np.random.normal(0, 0.05)

        return features, targets

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]

# --- Load Dataset and Plot Raw Data ---
dataset = SyntheticTimeSeriesDataset(num_samples=1000, seq_length=20, num_features=1)
raw_data = dataset.features[:, :, 0].numpy().flatten()

plt.figure(figsize=(10, 3))
plt.plot(raw_data, label="Synthetic Time Series Data")
plt.title("Raw Synthetic Time Series Data")
plt.xlabel("Time Step")
plt.ylabel("Signal Value")
plt.legend()
plt.show()

# --- Train/Test Split ---
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_set, test_set = random_split(dataset, [train_size, test_size])
train_loader = DataLoader(train_set, batch_size=64, shuffle=True)
test_loader = DataLoader(test_set, batch_size=1, shuffle=False)

# --- Define LSTM Model ---
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        return self.fc(out).view(-1)

# --- Training Setup ---
model = LSTMModel(input_size=1, hidden_size=64)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
epochs = 100
loss_history = []

# --- Training Loop ---
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        preds = model(batch_x)
        loss = criterion(preds, batch_y.view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    loss_history.append(avg_loss)
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# --- Plot Epoch vs Loss ---
plt.figure(figsize=(8, 4))
plt.plot(loss_history, label="Training Loss")
plt.title("Epoch vs Loss")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
plt.grid(True)
plt.show()

# --- Evaluation ---
model.eval()
preds, targets = [], []
with torch.no_grad():
    for x, y in test_loader:
        pred = model(x).item()
        preds.append(pred)
        targets.append(y.item())

# --- Plot Predictions vs Actuals ---
plt.figure(figsize=(10, 4))
plt.plot(targets, label='Actual')
plt.plot(preds, label='Predicted')
plt.legend()
plt.title("Test Set Predictions vs Actuals")
plt.xlabel("Sample Index")
plt.ylabel("Target Value")
plt.grid(True)
plt.show()

# --- Predict Future 5 Values (Auto-regressive) ---
last_input, _ = test_set[-1]
future_preds = []
current_input = last_input.unsqueeze(0)

model.eval()
with torch.no_grad():
    for _ in range(5):
        next_pred = model(current_input).view(1, 1, 1)  # reshape for next step
        future_preds.append(next_pred.item())
        current_input = torch.cat([current_input[:, 1:, :], next_pred], dim=1)

print("Next 5 predicted values after last input:")
print(future_preds)
======================================================================================================
for lstm basic version
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# --- Load Dataset ---
dataset = SyntheticTimeSeriesDataset(num_samples=1000, seq_length=20, num_features=1)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_set, batch_size=64, shuffle=True)
test_loader = DataLoader(test_set, batch_size=1, shuffle=False)

# --- Define LSTM Model ---
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)  # out: [batch, seq_len, hidden]
        out = out[:, -1, :]    # last time step
        return self.fc(out).view(-1)

# --- Training Setup ---
model = LSTMModel(input_size=1, hidden_size=64)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
epochs = 100

# --- Training Loop ---
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch_x, batch_y in train_loader:
        optimizer.zero_grad()
        preds = model(batch_x)
        loss = criterion(preds, batch_y.view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}")

# --- Evaluation ---
model.eval()
preds, targets = [], []
with torch.no_grad():
    for x, y in test_loader:
        pred = model(x).item()
        preds.append(pred)
        targets.append(y.item())

# --- Plot Results ---
plt.figure(figsize=(10, 5))
plt.plot(targets, label='Actual')
plt.plot(preds, label='Predicted')
plt.legend()
plt.title("LSTM Prediction on Synthetic Time Series")
plt.xlabel("Sample Index")
plt.ylabel("Target Value")
plt.show()

=============================================================================================
for rnn
‚úÖ Updated Model: RNNModel
python
Copy code
class RNNModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.rnn(x)             # out: [batch, seq_len, hidden_size]
        out = out[:, -1, :]              # take last time step
        return self.fc(out).view(-1)     # map to scalar output
üîÅ Everything else stays the same:
Training loop

Loss function

Evaluation

Plotting

üß™ Plug this model into your training pipeline like:
python
Copy code
model = RNNModel(input_size=1, hidden_size=64)

================================================================================================

import torch
import torch.nn as nn
import torch.optim as optim
import string

# --- 1. Character Vocabulary ---
letters = string.ascii_lowercase + " #"  # 26 lowercase + space + EOF
n_letters = len(letters)

# --- 2. One-hot Encoding ---
def ltt(ch):
    ans = torch.zeros(n_letters)
    ans[letters.find(ch)] = 1
    return ans

def getLine(s):
    return torch.cat([ltt(c).unsqueeze(0) for c in s], dim=0).view(len(s), 1, n_letters)

# --- 3. Data Preparation ---
data = "i love neural networks"
data = data.lower()
EOF = "#"
seq_len = len(data)
targets = torch.tensor([letters.find(c) for c in data[1:] + EOF])

# --- 4. LSTM Model ---
class MyLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(MyLSTM, self).__init__()
        self.LSTM = nn.LSTM(input_dim, hidden_dim)

    def forward(self, inp, hc):
        output, _ = self.LSTM(inp, hc)
        return output

hidden_dim = n_letters
model = MyLSTM(n_letters, hidden_dim)
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.CrossEntropyLoss()

# --- 5. Input Tensor ---
inp = torch.cat([ltt(c).unsqueeze(0) for c in data], dim=0).view(seq_len, 1, n_letters)

# --- 6. Training Loop ---
n_iters = 150
for itr in range(n_iters):
    model.zero_grad()
    h = torch.rand(1, 1, hidden_dim)
    c = torch.rand(1, 1, hidden_dim)
    output = model(inp, (h, c)).view(seq_len, n_letters)
    loss = loss_fn(output, targets)
    loss.backward()
    optimizer.step()
    if itr % 10 == 0:
        print(f"Iter {itr}: Loss = {loss.item():.4f}")

# --- 7. Prediction ---
def predict(s):
    print(f"\nInput: {s}")
    inp = getLine(s)
    h = torch.rand(1, 1, hidden_dim)
    c = torch.rand(1, 1, hidden_dim)
    out = model(inp, (h, c))
    top_idx = out[-1][0].topk(1)[1].item()
    print(f"Predicted next char: {letters[top_idx]}")
    return letters[top_idx]

# --- 8. Predict next character ---
predict("i love neu")
============================================================================================================


